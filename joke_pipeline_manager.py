
from typing import List, Dict, Any, Optional
from.openrouter_client import OpenRouterClient
from.plansearch_for_jokes import PlanSearcherForJokes 
from.llm_judge import LLMJudge 
from.novelty_checker import NoveltyChecker 

class JokePipelineManager:
    def __init__(self, openrouter_api_key: str, joke_corpus_path: Optional[str] = None):
        self.llm_client = OpenRouterClient(api_key=openrouter_api_key)
        self.plan_searcher = PlanSearcherForJokes(self.llm_client)
        self.llm_judge = LLMJudge(self.llm_client)
     
        self.novelty_checker = NoveltyChecker(self.llm_client, joke_corpus_path=joke_corpus_path) 
        
        self.last_run_transparency_data = {}

    def generate_and_evaluate_jokes(self,
                                    topic: str,
                                    user_llm_choices: Dict[str, str],
                                    num_top_jokes: int = 3,
                                    use_critique_refinement: bool = True) -> List]:
        print(f"Pipeline started for topic: {topic} with LLM choices: {user_llm_choices}")
        
        
        obs_model = user_llm_choices.get("observation", "google/gemma-3-27b") 
        plan_model = user_llm_choices.get("plan", "google/gemma-3-27b")
        joke_model = user_llm_choices.get("joke_instantiation", "deepseek/deepseek-v3")
        critique_model = user_llm_choices.get("critique", "mistralai/mistral-small") # Mistral Small is often low-cost or free tier
        judge_model = user_llm_choices.get("judge", "anthropic/claude-3.5-sonnet") # Sonnet 3.5 is good, check for free tiers or use a smaller free one
        novelty_llm_model = user_llm_choices.get("novelty_llm", "mistralai/mistral-small")

        candidate_jokes_details = self.plan_searcher.run_plansearch(
            topic=topic,
            observation_model=obs_model,
            plan_model=plan_model,
            joke_model=joke_model,
            critique_model=critique_model if use_critique_refinement else None,
            use_critique_refinement=use_critique_refinement,
            max_plans_to_develop=5,
            num_second_order_obs_per_combo=2
        )

        if not candidate_jokes_details:
            print("No candidate jokes generated by PLANSEARCH.")
            self.last_run_transparency_data = {
                'topic': topic, 'llm_choices': user_llm_choices, 'plansearch_data': self.plan_searcher.get_transparency_data(),
                'error': "No candidate jokes from PLANSEARCH"
            }
            return

        evaluated_jokes =
        for i, joke_detail in enumerate(candidate_jokes_details):
            joke_text = joke_detail["joke_text"]
           
            novelty_scores = self.novelty_checker.calculate_overall_novelty_score(
                joke_text,
                perceived_novelty_model_name=novelty_llm_model
            )
            joke_detail["novelty_scores"] = novelty_scores
            
            evaluation = self.llm_judge.evaluate_joke(joke_text, judge_model)
            joke_detail["evaluation"] = evaluation


            if evaluation and "overall_funniness" in evaluation:
                funniness_score = float(evaluation.get("overall_funniness", 0))
                scaled_novelty = novelty_scores.get("final_novelty_score", 0.0) * 10 
                
                combined_score = (0.7 * funniness_score) + (0.3 * scaled_novelty)
                joke_detail["combined_score"] = combined_score
                evaluated_jokes.append(joke_detail)
            else:
                joke_detail["combined_score"] = 0 
                evaluated_jokes.append(joke_detail)

        ranked_jokes = sorted(evaluated_jokes, key=lambda x: x.get("combined_score", 0), reverse=True)
        
        self.last_run_transparency_data = {
            'topic': topic, 
            'llm_choices': user_llm_choices,
            'plansearch_data': self.plan_searcher.get_transparency_data(),
            'novelty_checker_data': self.novelty_checker.get_transparency_data(),
            'llm_judge_data': self.llm_judge.get_transparency_data(),
            'final_ranked_jokes_summary': [{ "joke": jk["joke_text"][:60]+"...", "score": jk.get("combined_score")} for jk in ranked_jokes[:num_top_jokes]]
        }

        return ranked_jokes[:num_top_jokes]

    def get_last_run_transparency_data(self) -> Dict:
        return self.last_run_transparency_data